########################################################################################################################
# Results (3): Length of solution: 16 | Score: 1372 | Done: True | ['(board p1 slow0-0 n2 n0 n1)', '(move-down-slow slow0-0 n2 n1)', '(board p4 slow0-0 n1 n1 n2)', '(move-up-slow slow0-0 n1 n5)', '(leave p4 slow0-0 n5 n2 n1)', '(board p2 slow0-0 n5 n1 n2)', '(board p0 slow0-0 n5 n2 n3)', '(move-up-slow slow0-0 n5 n8)', '(leave p0 slow0-0 n8 n3 n2)', '(move-down-slow slow0-0 n8 n0)', '(board p3 slow0-0 n0 n2 n3)', '(move-up-slow slow0-0 n0 n6)', '(leave p3 slow0-0 n6 n3 n2)', '(leave p2 slow0-0 n6 n2 n1)', '(move-up-slow slow0-0 n6 n7)', '(leave p1 slow0-0 n7 n1 n0)']
# Results (4): Length of solution: 17 | Score: 1368 | Done: True | ['(move-down-slow slow0-0 n2 n1)', '(board p4 slow0-0 n1 n0 n1)', '(move-down-slow slow0-0 n1 n0)', '(board p3 slow0-0 n0 n1 n2)', '(move-up-slow slow0-0 n0 n5)', '(leave p4 slow0-0 n5 n2 n1)', '(board p0 slow0-0 n5 n1 n2)', '(board p2 slow0-0 n5 n2 n3)', '(move-up-slow slow0-0 n5 n6)', '(leave p3 slow0-0 n6 n3 n2)', '(leave p2 slow0-0 n6 n2 n1)', '(move-up-slow slow0-0 n6 n8)', '(leave p0 slow0-0 n8 n1 n0)', '(move-down-slow slow0-0 n8 n2)', '(board p1 slow0-0 n2 n0 n1)', '(move-up-slow slow0-0 n2 n7)', '(leave p1 slow0-0 n7 n1 n0)']
########################################################################################################################

RESOURCES_FOLDER = "/Users/csr95/Desktop/MSc_Artificial_Intelligence_HWU/MSc_Project_Dissertation/Combining-RL-Planning/Encoding_Module/Resources/"
FIGURES_FOLDER = "/Users/csr95/Desktop/MSc_Artificial_Intelligence_HWU/MSc_Project_Dissertation/Combining-RL-Planning/Figures/"

DOMAIN = "elevators"
PROBLEM = "elevators_p4"

# MODEL_NAME = ""

# Agent settings
REPLAY_MEMORY_SIZE = 20_000  # 50_000
MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps to start training
MINIBATCH_SIZE = 64  # The number of samples we use for training
UPDATE_TARGET_EVERY = 100
HARD_UPDATE = False  # True --> Hard update | False --> Soft update
TAU = 0.01
DISCOUNT = 0.99
LEARNING_RATE = 0.001

# Environment settings
GOAL_REWARD = 800
EPISODES = 4000
MAX_STEP_PER_EPISODE = 100
EPSILON_DECAY = 0.9975
MIN_EPSILON = 0.0001
# MAX_REWARD = 13

SHOW_STATS_EVERY = 25  # (Episodes)

NUMBER_OF_PREVIOUS_PLANS = 15  # 33
REDUCE_ACTION_SPACE = True   #Â False --> Alg. uses full action space
                             # True --> Alg. filters legal actions with the ones appearing on the prior plans whenever it's possible, otherwise it uses the full action space
REUSE_RATE = 0.25